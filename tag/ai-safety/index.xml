<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Safety | Matthew A. Clarke</title>
    <link>https://mclarke1991.github.io/tag/ai-safety/</link>
      <atom:link href="https://mclarke1991.github.io/tag/ai-safety/index.xml" rel="self" type="application/rss+xml" />
    <description>AI Safety</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 21 May 2025 16:37:45 +0100</lastBuildDate>
    <image>
      <url>https://mclarke1991.github.io/media/icon_hu10226378635250344412.png</url>
      <title>AI Safety</title>
      <link>https://mclarke1991.github.io/tag/ai-safety/</link>
    </image>
    
    <item>
      <title>Containing Spillover with Minimal Supervision</title>
      <link>https://mclarke1991.github.io/talk/containing-spillover-with-minimal-supervision/</link>
      <pubDate>Wed, 21 May 2025 16:37:45 +0100</pubDate>
      <guid>https://mclarke1991.github.io/talk/containing-spillover-with-minimal-supervision/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SAE Latent Co-occurrence</title>
      <link>https://mclarke1991.github.io/project/sae_cooccurrence/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://mclarke1991.github.io/project/sae_cooccurrence/</guid>
      <description>&lt;p&gt;Sparse AutoEncoder (SAE) latents show promise as a method for extracting interpretable features from large language models (LM), but their overall utility as the foundation of mechanistic understanding of LM remains unclear. Ideal features would be linear and independent, but we show that there exist SAE latents in GPT2-Small display non-independent behaviour, especially in small SAEs. Rather, they co-occur in clusters that map out interpretable subspaces. These subspaces show latents acting compositionally, as well as being used to resolve ambiguity in language. However, SAE latents remain largely independently interpretable within these contexts despite this behaviour. Furthermore, these clusters decrease in both size and prevalence as SAE width increases, suggesting this is a phenomenon of small SAEs with coarse-grained features.&lt;/p&gt;
&lt;p&gt;For more, see our &lt;a href=&#34;https://www.lesswrong.com/posts/WNoqEivcCSg8gJe5h/compositionality-and-ambiguity-latent-co-occurrence-and&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; on LessWrong, my talk as part of the &lt;a href=&#34;https://youtu.be/OyKXjdA09fE?si=gD9IRD0ohVMEbYwr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PIBBSS 2024 summer symposium&lt;/a&gt; or explore the &lt;a href=&#34;https://feature-cooccurrence.streamlit.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAE Latent Co-occurrence App&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Routing</title>
      <link>https://mclarke1991.github.io/project/gradient_routing/</link>
      <pubDate>Thu, 28 Mar 2024 16:25:37 +0000</pubDate>
      <guid>https://mclarke1991.github.io/project/gradient_routing/</guid>
      <description>&lt;p&gt;Can we control where learning happens in neural networks? Gradient routing addresses this by applying masks to limit the flow of gradients during backpropagation. By supplying different masks for different data points, the user can induce specialized subcomponents within a model. As part of SPAR, I worked on applying this method to AI safety research, focussing on addressing the problem of &lt;a href=&#34;https://arxiv.org/abs/2502.17424&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;emergent misalignment&lt;/a&gt; during &lt;a href=&#34;https://arxiv.org/abs/2302.00487&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continual learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Image credit: Adapted from &lt;a href=&#34;https://arxiv.org/abs/2410.04332&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud et al., 2024&lt;/a&gt;. &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC-BY 4.0&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
